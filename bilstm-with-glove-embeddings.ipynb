{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing pandas\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the dataset\ndf_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_train['id']\ndel df_train['location']\ndel df_test['id']\ndel df_test['location']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Denoising text","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport nltk\n!pip install contractions\nimport contractions\nimport re\nimport string\nimport unicodedata\n!pip install inflect\nimport inflect\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#denoising text\ndef denoise_text(text):\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = contractions.fix(text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\ndef replace_numbers(words):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\ndef normalize_text(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = replace_numbers(words)\n    words = remove_stopwords(words)\n    words = lemmatize_verbs(words)\n    return words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize tweet into words\ndef tokenize(text):\n    return nltk.word_tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize tweet into words\ndef tokenize(text):\n    return nltk.word_tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_prepare(text):\n    text = denoise_text(text)\n    text = ' '.join([x for x in normalize_text(tokenize(text))])\n    return text\ndf_train['text'] = [text_prepare(x) for x in df_train['text']]\ndf_test['text'] = [text_prepare(x) for x in df_test['text']]\n\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport logging\nlogging.basicConfig(level=logging.INFO)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests, zipfile, io\nzip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_model_input(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=100):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    text = text[indices]\n    print(text.shape)\n    X_train_Glove = text[0:len(X_train), ]\n    X_test_Glove = text[len(X_train):, ]\n    embeddings_dict = {}\n    f = open(\"glove.6B.100d.txt\", encoding=\"utf8\")\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_dict[word] = coefs\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_dict))\n    return (X_train_Glove, X_test_Glove, word_index, embeddings_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_bilstm(word_index, embeddings_dict, nclasses,  MAX_SEQUENCE_LENGTH=100, EMBEDDING_DIM=100, dropout=0.5, hidden_layer = 3, lstm_node = 32):\n    # Initialize a sequential model\n    model = Sequential()\n    # Make the embedding matrix using the embedding_dict\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_dict.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) != len(embedding_vector):\n                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n            \n    # Add embedding layer\n    model.add(Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True))\n    # Add hidden layers \n    for i in range(0,hidden_layer):\n        # Add a bidirectional lstm layer\n        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.2)))\n        # Add a dropout layer after each lstm layer\n        model.add(Dropout(dropout))\n    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.2)))\n    model.add(Dropout(dropout))\n    # Add the fully connected layer with 256 neurons and relu activation\n    model.add(Dense(256, activation='relu', kernel_regularizer = regularizers.L2(0.001)))\n    # Add the output layer with softmax activation since we have 2 classes\n    model.add(Dense(nclasses, activation='sigmoid'))\n    # Compile the model using sparse_categorical_crossentropy\n    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(loss='binary_crossentropy',\n                      optimizer=optimizer,\n                      metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.text\ny = df_train.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nX_train_Glove, X_test_Glove, word_index, embeddings_dict = prepare_model_input(X_train,X_test)\nmodel = build_bilstm(word_index, embeddings_dict, 1)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)/(tp+fp)\n    recall = (tp)/(tp+fn)\n    f1 = (2*(precision*recall))/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"true positive\": tp,\n        \"true negative\": tn,\n        \"false positive\": fp,\n        \"false negative\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_Glove, y_train,\n                           validation_data=(X_test_Glove,y_test),\n                           epochs=3,\n                           batch_size=128,\n                           verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Evaluating Model ... \\n\")\npredicted = (model.predict(X_test_Glove) > 0.5).astype(\"int32\")\n\nprint(metrics.classification_report(y_test, predicted))\nprint(\"\\n\")\nlogger = logging.getLogger(\"logger\")\nresult = compute_metrics(y_test, predicted)\nfor key in (result.keys()):\n    logger.info(\"  %s = %s\", key, str(result[key]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = np.array(df_test['text'])\ntokenizer1 = Tokenizer(num_words=75000)\ntokenizer1.fit_on_texts(text1)\nsequences1 = tokenizer1.texts_to_sequences(text1)\nword_index1 = tokenizer1.word_index\ntext1 = pad_sequences(sequences1, maxlen=100)\nindices1 = np.arange(text1.shape[0])\ntext1 = text1[indices1]\npredict = (model.predict(text1) > 0.5).astype(\"int32\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsample['target']= (predict>0.5).astype(int)\nsample.to_csv(\"./submission.csv\",index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}